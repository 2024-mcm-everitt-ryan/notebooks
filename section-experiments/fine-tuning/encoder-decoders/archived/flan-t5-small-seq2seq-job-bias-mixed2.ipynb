{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6a5f862663af8b",
   "metadata": {},
   "source": [
    "Adapted from the following, but changed to handle multi-label\n",
    "https://github.com/VanekPetr/flan-t5-text-classifier/blob/main/classifier/AutoModelForSeq2SeqLM/flan-t5-finetuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f047a5028d7079",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "d12b7ee6dc5dc0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:33:07.436683Z",
     "start_time": "2024-07-06T11:33:07.427330Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_model_id = 'google/flan-t5-small'\n",
    "\n",
    "hf_site_id = '2024-mcm-everitt-ryan'\n",
    "dataset_id = f'{hf_site_id}/job-bias-synthetic-human-benchmark'\n",
    "#dataset_id = f'{hf_site_id}/job-bias-synthetic-human-verified'\n",
    "\n",
    "\n",
    "base_model_name = base_model_id.split('/')[-1]\n",
    "model_id = f'{base_model_name}-seq2seq-job-bias-mixed'\n",
    "hub_model_id = f'{hf_site_id}/{model_id}'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "dcebcc367d1edf29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:33:07.450031Z",
     "start_time": "2024-07-06T11:33:07.447527Z"
    }
   },
   "source": [
    "\n",
    "seed=2024\n",
    "\n",
    "# Training\n",
    "num_train_epochs=10\n",
    "batch_size=8\n",
    "learning_rate=3e-4\n",
    "#learning_rate = 5e-5\n",
    "\n",
    "# Regularisation\n",
    "dropout_rate = 0.1\n",
    "weight_decay=0.0001\n",
    "\n",
    "# Misc\n",
    "results_output_dir = 'results'\n",
    "logging_dir='logs'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "e9c27564-3917-477d-a69f-09fdeed244c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T20:22:34.465271Z",
     "start_time": "2024-07-04T20:22:32.038995Z"
    }
   },
   "source": [
    "!pip install -q transformers datasets sentencepiece accelerate evaluate hf_transfer huggingface_hub scikit-learn protobuf nltk"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "bc121d59bab23a36",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "ddb7a63b9d9fcb8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:34.960391Z",
     "start_time": "2024-07-06T11:34:32.007371Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_id)\n",
    "column_names = dataset['train'].column_names\n",
    "\n",
    "\n",
    "text_col = 'text'\n",
    "label_cols = [col for col in column_names if col.startswith('label_')]\n",
    "\n",
    "labels = [label.replace(\"label_\", \"\") for label in label_cols]\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# Remove all columns apart from the two needed for multi-class classification\n",
    "keep_columns = ['id', text_col] + label_cols\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    dataset[split] = dataset[split].remove_columns(\n",
    "        [col for col in dataset[split].column_names if col not in keep_columns])\n",
    "\n",
    "for type in ['train','val','test']:\n",
    "    dataset[type] = dataset[type].shuffle(seed=seed).select(range(10))\n",
    "\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label_age', 'label_disability', 'label_masculine', 'label_feminine', 'label_racial', 'label_sexuality', 'label_general', 'text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'label_age', 'label_disability', 'label_masculine', 'label_feminine', 'label_racial', 'label_sexuality', 'label_general', 'text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label_age', 'label_disability', 'label_masculine', 'label_feminine', 'label_racial', 'label_sexuality', 'label_general', 'text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokeniser",
   "id": "e81dba26cb2d67f1"
  },
  {
   "cell_type": "code",
   "id": "df1730b90a1dcc7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:37.399880Z",
     "start_time": "2024-07-06T11:34:37.236923Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='google/flan-t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "54abe108f979a066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:37.526901Z",
     "start_time": "2024-07-06T11:34:37.494784Z"
    }
   },
   "source": [
    "from datasets import concatenate_datasets\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# Prepare target sequences for T5\n",
    "def create_target_sequence(example):\n",
    "    labels = [key.replace('label_','') for key, value in example.items() if key.startswith('label_') and value]\n",
    "    labels = ','.join(labels)\n",
    "    labels = labels.strip()    \n",
    "    return labels\n",
    "\n",
    "# Add target sequence to the dataset\n",
    "dataset = dataset.map(lambda x: {'labels': create_target_sequence(x)}, remove_columns=[col for col in dataset['train'].column_names if col.startswith('label_')])\n",
    "\n",
    "# Tokenise targets\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"labels\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 512\n",
      "Max target length: 9\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "2917c7f3b405da7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:37.625692Z",
     "start_time": "2024-07-06T11:34:37.623287Z"
    }
   },
   "source": [
    "#tokenized_targets[\"input_ids\"]"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "98424c47a6f9843b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "102c5e2fda51da82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:40.684831Z",
     "start_time": "2024-07-06T11:34:37.936220Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(base_model_id, dropout_rate=dropout_rate)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    config=config\n",
    ")\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "7b1c50728f2cbab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:40.689759Z",
     "start_time": "2024-07-06T11:34:40.686101Z"
    }
   },
   "source": [
    "model.config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"google/flan-t5-small\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 1024,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"gelu_new\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 8,\n",
       "  \"num_heads\": 6,\n",
       "  \"num_layers\": 8,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.42.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "98d8c8d87d17cd44",
   "metadata": {},
   "source": [
    "# Preprocessing/Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:40.705861Z",
     "start_time": "2024-07-06T11:34:40.690650Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, \\\n",
    "    classification_report, confusion_matrix\n",
    "import nltk\n",
    "from transformers import  DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from typing import List, Tuple\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_function(sample: Dataset, padding: str = \"max_length\") -> dict:\n",
    "    \"\"\"Preprocess the dataset.\"\"\"\n",
    "    inputs = [item for item in sample[\"text\"]]\n",
    "    labels = [item for item in sample[\"labels\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_source_length, padding=padding, truncation=True\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=labels, max_length=max_target_length, padding=padding, truncation=True\n",
    "    )\n",
    "\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(la if la != tokenizer.pad_token_id else -100) for la in label]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def postprocess_text(labels: List[str], preds: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Helper function to postprocess text\"\"\"\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "    return labels, preds\n",
    "\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    \n",
    "    y_hat, y = eval_predictions\n",
    "    \n",
    "    # Replace -100 in the labels .\n",
    "    y = np.where(y != -100, y, tokenizer.pad_token_id)\n",
    "    \n",
    "    if isinstance(y_hat, tuple):\n",
    "        y_hat = y_hat[0]\n",
    "        \n",
    "    print(y)\n",
    "    print('--------------------')\n",
    "    print(y_hat)\n",
    "    \n",
    "    y_str = tokenizer.batch_decode(y, skip_special_tokens=True)\n",
    "    y_hat_str = tokenizer.batch_decode(y_hat, skip_special_tokens=True)\n",
    "\n",
    "    y_str, y_hat_str = postprocess_text( y_str, y_hat_str)\n",
    "    \n",
    "    print('--------------------')\n",
    "    print(y_str)\n",
    "    print('--------------------')\n",
    "    print(y_hat_str)\n",
    "    print('--------------------')\n",
    "\n",
    "\n",
    "    # Flatten the list of labels\n",
    "    true_flat = [label.strip() for sublist in [t.split(',') for t in y_str] for label in sublist]\n",
    "    pred_flat = [label.strip() for sublist in [p.split(',') for p in y_hat_str] for label in sublist]\n",
    "    \n",
    "    \n",
    "    #print(true_flat)\n",
    "    #print('--------------------')\n",
    "    #print(pred_flat)\n",
    "\n",
    "    # Convert to binary format for multi-label metrics\n",
    "    #unique_labels = list(set(true_flat + pred_flat))  # This will include out-of-scope (not a label) predictions\n",
    "    unique_labels = list(set(true_flat))\n",
    "    \n",
    "    # Remove the blank label (no bias)\n",
    "    unique_labels = [label for label in unique_labels if label != '' and label is not None]\n",
    "    \n",
    "    y_true = [[1 if label in t else 0 for label in unique_labels] for t in y_str]\n",
    "    y_pred = [[1 if label in p else 0 for label in unique_labels] for p in y_hat_str]\n",
    "\n",
    "\n",
    "    y_true_str = [[label if label in t else 0 for label in unique_labels] for t in y_str]\n",
    "    y_pred_str = [[label if label in p else 0 for label in unique_labels] for p in y_hat_str]\n",
    "    #unique_labels = ['no_bias' if not label else label for label in unique_labels]\n",
    "\n",
    "    print('\\n------------------ Confusion Matrix ------------------')\n",
    "    conf_matrix = confusion_matrix(np.asarray(y_true).argmax(axis=1), np.asarray(y_pred).argmax(axis=1))\n",
    "    df_cm = pd.DataFrame(conf_matrix, index=unique_labels, columns=unique_labels)\n",
    "    print(df_cm)\n",
    "    print('\\n--------- Classification Report ------------------')\n",
    "    print(classification_report(y_true, y_pred, target_names=unique_labels))#, target_names=list(id2label.values())))\n",
    "\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    f1_samples = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "\n",
    "    precision_micro = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc_micro = roc_auc_score(y_true=y_true, y_score=y_pred, average='micro')\n",
    "    \n",
    "    \n",
    "    #for i in range(1): #range(len(y_true_str)):\n",
    "    #    yt  = [num for num in y_true_str[i] if num != 0]\n",
    "    #    yth  = [num for num in y_pred_str[i] if num != 0]\n",
    "    #    print(f't: {yt}')\n",
    "    #    print(f'p: {yth}')\n",
    "    #    print(f't: {y_true[i]}')\n",
    "    #    print(f'p: {y_pred[i]}')\n",
    "    #    print(f\"accuracy: {accuracy_score(y_true=y_true[i], y_pred=y_pred[i])}\")\n",
    "    #    print(f\"precision: {precision_score(y_true=y_true[i], y_pred=y_pred[i], average='micro')}\")\n",
    "    #    print(f\"recall: {recall_score(y_true=y_true[i], y_pred=y_pred[i], average='micro')}\")\n",
    "    #    print('----------------------')\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        f'f1_micro': f1_micro,\n",
    "        f'f1_macro': f1_macro,\n",
    "        f'f1_samples': f1_samples,\n",
    "        f'f1_weighted': f1_weighted,\n",
    "        f'precision_micro': precision_micro,\n",
    "        f'recall_micro': recall_micro,\n",
    "        f'roc_auc_micro': roc_auc_micro}\n",
    "    return metrics\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "24f54a078782e3d2",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "id": "62c7dcb25c557604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:40.750614Z",
     "start_time": "2024-07-06T11:34:40.707326Z"
    }
   },
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=results_output_dir,\n",
    "    #logging_dir=logging_dir,  # logging & evaluation strategies\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # Overflows with fp16\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "   # weight_decay=weight_decay\n",
    "    #report_to=\"tensorboard\",\n",
    "    #push_to_hub=True,\n",
    "    #hub_strategy=\"every_save\",\n",
    "    #hub_model_id=REPOSITORY_ID,\n",
    "    #hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"text\", \"labels\"]\n",
    ")\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=label_pad_token_id, pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "#early_stop = transformers.EarlyStoppingCallback(10, 1.15)\n",
    "class PrintClassificationCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['id', 'labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/teveritt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "594875a2793a8f29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T11:34:48.658237Z",
     "start_time": "2024-07-06T11:34:40.751115Z"
    }
   },
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrintClassificationCallback]\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # Silence the warnings.\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9/20 00:06 < 00:09, 1.14 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Samples</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>Roc Auc Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.766400</td>\n",
       "      <td>4.776971</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.924600</td>\n",
       "      <td>3.709516</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.837300</td>\n",
       "      <td>3.107040</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.522100</td>\n",
       "      <td>2.819735</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 6949   485     1     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  879     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [10050     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "[[    0     3     9  3452   533    11 11131   478     6   379  1035     6\n",
      "   4814     6    11  2267  2591     1     0     0]\n",
      " [    0     3 26727   225  4237    70  4258    11  1189  2068 20185    70\n",
      "   2193   351    12 13325  1741   189    32  2754]\n",
      " [    0  1079  4411     6 14735    11  2621     7    19     3     9     3\n",
      "  13003 22963  1669  2425    12     3  6930   306]\n",
      " [    0     3     9  4896    11 12758   161  1164     1     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0 10605     6 29262    11  6193    19     3     9     3 16921  9157\n",
      "   1669     3     7 26253    16  1260  3452   268]\n",
      " [    0     3 26727   398    36     3   179    12  1581    21     8  1102\n",
      "      5     1     0     0     0     0     0     0]\n",
      " [    0     3 15829  8313   138    56  1099    21  4311  4717 12505    28\n",
      "   4336 27771    16     3     9  3107  4700    28]\n",
      " [    0 20429   276  6480    19     3     9  1374     3    17 18329     7\n",
      "    349  2425    12     3  6930  3058  1275    12]\n",
      " [    0     3 26727   398    43     3     9  1101  2458    16  4850  3867\n",
      "     11   605   758     6    28  1287 13477    11]\n",
      " [    0     3     9   423    18   715  1102    13   220 15731     1     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "['', 'sexuality', 'general', 'feminine', 'feminine', '', '', 'racial', 'racial', 'disability']\n",
      "--------------------\n",
      "['a comprehensive health and wellness program, including medical, dental, and vision coverage', 'Applicants should submit their resume and cover letter detailing their relevant experience to careers@thomas', 'Johnston, Bernard and Powers is a prestigious consultancy firm dedicated to delivering high', 'a dynamic and supportive work environment', 'Kelly, Bentley and Jones is a reputable consulting firm specializing in providing comprehensive business', 'Applicants must be able to apply for the position.', 'NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with', 'Harrison PLC is a leading telecommunications company dedicated to delivering innovative solutions to', 'Applicants must have a strong background in electrical engineering and event management, with excellent organizational and', 'a full-time position of 37.5']\n",
      "--------------------\n",
      "\n",
      "------------------ Confusion Matrix ------------------\n",
      "            general  racial  feminine  sexuality  disability\n",
      "general           4       0         0          0           0\n",
      "racial            2       0         0          0           0\n",
      "feminine          2       0         0          0           0\n",
      "sexuality         1       0         0          0           0\n",
      "disability        1       0         0          0           0\n",
      "\n",
      "--------- Classification Report ------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     general       0.00      0.00      0.00         1\n",
      "      racial       0.00      0.00      0.00         2\n",
      "    feminine       0.00      0.00      0.00         2\n",
      "   sexuality       0.00      0.00      0.00         1\n",
      "  disability       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         7\n",
      "   macro avg       0.00      0.00      0.00         7\n",
      "weighted avg       0.00      0.00      0.00         7\n",
      " samples avg       0.00      0.00      0.00         7\n",
      "\n",
      "----------------------------------------------------------\n",
      "[[    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 6949   485     1     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  879     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [10050     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "[[    0     3     9  3452   533    11 11131   478     1     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3 26727   225  4237    70  4258    11  1189  2068 20185    70\n",
      "   2193   351    12 13325  1741   189    32  2754]\n",
      " [    0     3     9   306    18  4497 20184  3268  7792     1     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  4896    11 12758   161  1164     1     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9   423    18   715   613     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3 26727   398    36     3     9  5213    13     3     9 16428\n",
      "     31     7  1952    16     3     9  2193  1057]\n",
      " [    0     3 15829  8313   138    56  1099    21  4311  4717 12505    28\n",
      "   4336 27771    16     3     9  3107  4700    28]\n",
      " [    0 20429   276  6480    19     3     9  1374     3    17 18329     7\n",
      "    349  2425    12     3  6930  3058  1275    12]\n",
      " [    0     3     9  1101  2458    16  4850  3867    42     3     9  1341\n",
      "   1057     1     0     0     0     0     0     0]\n",
      " [    0     3     9  2559    13   220 15731   716     1     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "['', 'sexuality', 'general', 'feminine', 'feminine', '', '', 'racial', 'racial', 'disability']\n",
      "--------------------\n",
      "['a comprehensive health and wellness program', 'Applicants should submit their resume and cover letter detailing their relevant experience to careers@thomas', 'a high-quality investigative consultant', 'a dynamic and supportive work environment', 'a full-time job', \"Applicants must be a graduate of a bachelor's degree in a relevant field\", 'NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with', 'Harrison PLC is a leading telecommunications company dedicated to delivering innovative solutions to', 'a strong background in electrical engineering or a related field', 'a minimum of 37.5 hours']\n",
      "--------------------\n",
      "\n",
      "------------------ Confusion Matrix ------------------\n",
      "            general  racial  feminine  sexuality  disability\n",
      "general           4       0         0          0           0\n",
      "racial            2       0         0          0           0\n",
      "feminine          2       0         0          0           0\n",
      "sexuality         1       0         0          0           0\n",
      "disability        1       0         0          0           0\n",
      "\n",
      "--------- Classification Report ------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     general       0.00      0.00      0.00         1\n",
      "      racial       0.00      0.00      0.00         2\n",
      "    feminine       0.00      0.00      0.00         2\n",
      "   sexuality       0.00      0.00      0.00         1\n",
      "  disability       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         7\n",
      "   macro avg       0.00      0.00      0.00         7\n",
      "weighted avg       0.00      0.00      0.00         7\n",
      " samples avg       0.00      0.00      0.00         7\n",
      "\n",
      "----------------------------------------------------------\n",
      "[[    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 6949   485     1     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  879     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [10050     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "[[    0     3     9 16428    31     7  1952     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3 26727   225  4237    70  4258    11  1189  2068 20185    70\n",
      "   2193   351    12 13325  1741   189    32  2754]\n",
      " [    0     3     9   137     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  4896    11 12758   161  1164     1     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  2335     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3 26727   398    36     3     9  5213    13     3     9 16428\n",
      "     31     7  1952     1     0     0     0     0]\n",
      " [    0     3 15829  8313   138    56  1099    21  4311  4717 12505    28\n",
      "   4336 27771    16     3     9  3107  4700    28]\n",
      " [    0     3     9  2399    11 13066   372     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  1101  2458    16  4850  3867     1     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  2559    13   220 15731     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "['', 'sexuality', 'general', 'feminine', 'feminine', '', '', 'racial', 'racial', 'disability']\n",
      "--------------------\n",
      "[\"a bachelor's degree\", 'Applicants should submit their resume and cover letter detailing their relevant experience to careers@thomas', 'a).', 'a dynamic and supportive work environment', 'a woman', \"Applicants must be a graduate of a bachelor's degree\", 'NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with', 'a diverse and inclusive team', 'a strong background in electrical engineering', 'a minimum of 37.5']\n",
      "--------------------\n",
      "\n",
      "------------------ Confusion Matrix ------------------\n",
      "            general  racial  feminine  sexuality  disability\n",
      "general           4       0         0          0           0\n",
      "racial            2       0         0          0           0\n",
      "feminine          2       0         0          0           0\n",
      "sexuality         1       0         0          0           0\n",
      "disability        1       0         0          0           0\n",
      "\n",
      "--------- Classification Report ------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     general       0.00      0.00      0.00         1\n",
      "      racial       0.00      0.00      0.00         2\n",
      "    feminine       0.00      0.00      0.00         2\n",
      "   sexuality       0.00      0.00      0.00         1\n",
      "  disability       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         7\n",
      "   macro avg       0.00      0.00      0.00         7\n",
      "weighted avg       0.00      0.00      0.00         7\n",
      " samples avg       0.00      0.00      0.00         7\n",
      "\n",
      "----------------------------------------------------------\n",
      "[[    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 6949   485     1     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  879     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [21546     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    1     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    3    52     9  4703     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [10050     1     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "[[    0     3     9 16428    31     7  1952     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9    26    32    40 21282     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9   137     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9   793 17997     1     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  2335     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3 26727   398    43     3     9 16428    31     7  1952     1\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9   137     1     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9    26    32    40 11719     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  1101  2458    16  4850  3867     1     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    0     3     9  2559    13   220 15731     1     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "--------------------\n",
      "['', 'sexuality', 'general', 'feminine', 'feminine', '', '', 'racial', 'racial', 'disability']\n",
      "--------------------\n",
      "[\"a bachelor's degree\", 'adolescence', 'a).', 'a natural caregiver', 'a woman', \"Applicants must have a bachelor's degree\", 'a).', 'adolescent', 'a strong background in electrical engineering', 'a minimum of 37.5']\n",
      "--------------------\n",
      "\n",
      "------------------ Confusion Matrix ------------------\n",
      "            general  racial  feminine  sexuality  disability\n",
      "general           4       0         0          0           0\n",
      "racial            2       0         0          0           0\n",
      "feminine          2       0         0          0           0\n",
      "sexuality         1       0         0          0           0\n",
      "disability        1       0         0          0           0\n",
      "\n",
      "--------- Classification Report ------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     general       0.00      0.00      0.00         1\n",
      "      racial       0.00      0.00      0.00         2\n",
      "    feminine       0.00      0.00      0.00         2\n",
      "   sexuality       0.00      0.00      0.00         1\n",
      "  disability       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         7\n",
      "   macro avg       0.00      0.00      0.00         7\n",
      "weighted avg       0.00      0.00      0.00         7\n",
      " samples avg       0.00      0.00      0.00         7\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_168156/2461148478.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_cache\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m  \u001B[0;31m# Silence the warnings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/local/venv/dcu-ai/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1930\u001B[0m                 \u001B[0mhf_hub_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_progress_bars\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1931\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1932\u001B[0;31m             return inner_training_loop(\n\u001B[0m\u001B[1;32m   1933\u001B[0m                 \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1934\u001B[0m                 \u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/local/venv/dcu-ai/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2364\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallback_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_epoch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2365\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_log_save_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtr_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_keys_for_eval\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2366\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2367\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mDebugOption\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTPU_METRICS_DEBUG\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/local/venv/dcu-ai/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_maybe_log_save_evaluate\u001B[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2794\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2795\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_save\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2796\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_save_checkpoint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2797\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallback_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_save\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2798\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/local/venv/dcu-ai/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_save_checkpoint\u001B[0;34m(self, model, trial, metrics)\u001B[0m\n\u001B[1;32m   2877\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_only_model\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2878\u001B[0m             \u001B[0;31m# Save optimizer and scheduler\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2879\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_save_optimizer_and_scheduler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2880\u001B[0m             \u001B[0;31m# Save RNG state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2881\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_save_rng_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/local/venv/dcu-ai/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_save_optimizer_and_scheduler\u001B[0;34m(self, output_dir)\u001B[0m\n\u001B[1;32m   2993\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_save\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2994\u001B[0m             \u001B[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2995\u001B[0;31m             \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mOPTIMIZER_NAME\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2996\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2997\u001B[0m         \u001B[0;31m# Save SCHEDULER & SCALER\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/local/venv/dcu-ai/lib/python3.10/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m    627\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_use_new_zipfile_serialization\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    628\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0m_open_zipfile_writer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mopened_zipfile\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 629\u001B[0;31m             \u001B[0m_save\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopened_zipfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_disable_byteorder_record\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    630\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    631\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/local/venv/dcu-ai/lib/python3.10/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_save\u001B[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m    861\u001B[0m         \u001B[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    862\u001B[0m         \u001B[0mnum_bytes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstorage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnbytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 863\u001B[0;31m         \u001B[0mzip_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite_record\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata_ptr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_bytes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    864\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    865\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "5bddaea63cf3ddc0",
   "metadata": {},
   "source": [
    "test_results = trainer.evaluate(eval_dataset=tokenized_dataset['test'])\n",
    "test_results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b62e5ed98e05426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:15:26.542521Z",
     "start_time": "2024-07-04T15:15:26.536395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric     Value\n",
      "              eval_loss  0.810158\n",
      "          eval_accuracy  0.590693\n",
      "          eval_f1_micro  0.517121\n",
      "          eval_f1_macro  0.546836\n",
      "        eval_f1_samples  0.293357\n",
      "       eval_f1_weighted  0.545981\n",
      "   eval_precision_micro  0.427252\n",
      "      eval_recall_micro  0.654867\n",
      "     eval_roc_auc_micro  0.790995\n",
      "           eval_runtime 18.095300\n",
      "eval_samples_per_second 58.192000\n",
      "  eval_steps_per_second  7.295000\n",
      "                  epoch 10.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(test_results.items()), columns=['Metric', 'Value'])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "843e92a89b8b7b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:15:26.573002Z",
     "start_time": "2024-07-04T15:15:26.543130Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def classify_text(text, model, tokenizer, label_columns, device):\n",
    "    input_text = f\"classify: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    predicted_labels = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicted_labels = [label.strip() for label in predicted_labels.split(',')]\n",
    "    label_dict = {label: False for label in label_columns}\n",
    "    for label in predicted_labels:\n",
    "        if label in label_dict:\n",
    "            label_dict[label] = True\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d76dd95a6c47d93f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:15:26.594631Z",
     "start_time": "2024-07-04T15:15:26.573803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': False,\n",
       " 'disability': False,\n",
       " 'masculine': False,\n",
       " 'feminine': False,\n",
       " 'racial': True,\n",
       " 'sexuality': False,\n",
       " 'general': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Looking for a native English speaker\"\n",
    "\n",
    "classify_text(text, model, tokenizer, labels, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1528ecf88ae8e59",
   "metadata": {},
   "source": [
    "# Pushing to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb013231800de2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a3e3a4de9340f6b33bb52e2133a4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab570ca8f2f2497b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e618b47cb64f4cb0dd72c5c8645181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de2bbec40f14cd79964e862325be01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becebb6264c14947a17c7d1fec8e33d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/2024-mcm-everitt-ryan/flan-t5-small-seq2seq-job-bias-mixed/commit/6c498663facc8e84326823f0f534fde2db2718de', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='6c498663facc8e84326823f0f534fde2db2718de', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import ModelCard, EvalResult, ModelCardData, HfFolder\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "\n",
    "model.push_to_hub(repo_id=hub_model_id, token=HfFolder.get_token())\n",
    "tokenizer.push_to_hub(repo_id=hub_model_id, token=HfFolder.get_token())\n",
    "\n",
    "###### Update Model Card ######\n",
    "\n",
    "eval_results = []\n",
    "for k, v in test_results.items():\n",
    "    eval_results.append(EvalResult(\n",
    "        task_type='multi_label_classification',\n",
    "        dataset_type='mix_human-eval_synthetic',\n",
    "        dataset_name=dataset_id,\n",
    "        metric_type=k.replace(\"eval_\", \"\", 1),\n",
    "        metric_value=v))\n",
    "\n",
    "direct_use = \"\"\"\n",
    "    ```python\n",
    "    from transformers import pipeline\n",
    "\n",
    "    pipe = pipeline(\"text-classification\", model=\"${hub_model_id}\", return_all_scores=True)\n",
    "\n",
    "    results = pipe(\"Join our dynamic and fast-paced team as a Junior Marketing Specialist. We seek a tech-savvy and energetic individual who thrives in a vibrant environment. Ideal candidates are digital natives with a fresh perspective, ready to adapt quickly to new trends. You should have recent experience in social media strategies and a strong understanding of current digital marketing tools. We're looking for someone with a youthful mindset, eager to bring innovative ideas to our young and ambitious team. If you're a recent graduate or early in your career, this opportunity is perfect for you!\")\n",
    "    print(results)\n",
    "    ```\n",
    "    >> [[\n",
    "    {'label': 'age', 'score': 0.9883460402488708}, \n",
    "    {'label': 'disability', 'score': 0.00787709467113018}, \n",
    "    {'label': 'feminine', 'score': 0.007224376779049635}, \n",
    "    {'label': 'general', 'score': 0.09967829287052155}, \n",
    "    {'label': 'masculine', 'score': 0.0035264550242573023}, \n",
    "    {'label': 'racial', 'score': 0.014618005603551865}, \n",
    "    {'label': 'sexuality', 'score': 0.005568435415625572}\n",
    "    ]]\n",
    "    \"\"\"\n",
    "direct_use = direct_use.replace('${hub_model_id}', hub_model_id, -1)\n",
    "\n",
    "card_data = ModelCardData(\n",
    "    model_id=model_id,\n",
    "    model_name=model_id,\n",
    "    model_description=\"The model is a multi-label classifier designed to detect various types of bias within job descriptions.\",\n",
    "    base_model=base_model_id,\n",
    "    language='en',\n",
    "    license='apache-2.0',\n",
    "    developers=\"Tristan Everitt and Paul Ryan\",\n",
    "    model_card_authors='See developers',\n",
    "    model_card_contact='See developers',\n",
    "    repo=\"https://gitlab.computing.dcu.ie/everitt2/2024-mcm-everitt-ryan\",\n",
    "    eval_results=eval_results,\n",
    "    compute_infrastructure=f'{platform.system()} {platform.release()} {platform.processor()}',\n",
    "    # hardware_requirements=f\"CPUs: {psutil.cpu_count()}, Memory: {psutil.virtual_memory().total} bytes\",\n",
    "    software=f'Python {platform.python_version()}',\n",
    "    hardware_type=platform.machine(),\n",
    "    hours_used='N/A',\n",
    "    cloud_provider='N/A',\n",
    "    cloud_region='N/A',\n",
    "    co2_emitted='N/A',\n",
    "    datasets=[dataset_id],\n",
    "    direct_use=direct_use\n",
    ")\n",
    "\n",
    "card = ModelCard.from_template(card_data)\n",
    "\n",
    "card.push_to_hub(repo_id=hub_model_id, token=HfFolder.get_token())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DCU AI)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
