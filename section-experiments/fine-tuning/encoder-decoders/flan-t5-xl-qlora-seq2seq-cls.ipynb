{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6a5f862663af8b",
   "metadata": {},
   "source": [
    "Adapted from the following, but changed to handle multi-label\n",
    "https://github.com/VanekPetr/flan-t5-text-classifier/blob/main/classifier/AutoModelForSeq2SeqLM/flan-t5-finetuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78b8b3ad0b097a",
   "metadata": {},
   "source": [
    "# Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9c27564-3917-477d-a69f-09fdeed244c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:08.214625Z",
     "start_time": "2024-07-16T12:40:08.212434Z"
    }
   },
   "source": "#!pip install -q transformers datasets sentencepiece accelerate evaluate peft bitsandbytes protobuf hf_transfer scikit-learn nltk tiktoken huggingface_hub",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3d918dba9ec9b61a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:08.414553Z",
     "start_time": "2024-07-16T12:40:08.252699Z"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "# Setup"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6950d2457354983bd867fe63bc5eca9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e59b773a-88de-45cd-9742-eefb36c3b606",
   "metadata": {},
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_model_id = 'google/flan-t5-xl'\n",
    "\n",
    "seed = 2024\n",
    "\n",
    "use_lora = True\n",
    "use_quantization = use_lora and True\n",
    "use_fp16 = not use_quantization and True\n",
    "\n",
    "# Training\n",
    "num_train_epochs = 3\n",
    "batch_size = 8\n",
    "\n",
    "use_gradient_checkpointing = False,  # Save some memory at the expense of training\n",
    "# See https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "#learning_rate = 5e-5\n",
    "#learning_rate=3e-4\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Regularisation\n",
    "dropout_rate = 0.1\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Evaluation\n",
    "label_threshold = 0.5\n",
    "\n",
    "# Misc\n",
    "results_output_dir = 'results'\n",
    "logging_dir = 'logs'\n",
    "\n",
    "hf_site_id = '2024-mcm-everitt-ryan'\n",
    "dataset_id = f'{hf_site_id}/job-bias-synthetic-human-benchmark-v2'\n",
    "#dataset_id = f'{hf_site_id}/job-bias-synthetic-human-verified'\n",
    "base_model_name = base_model_id.split('/')[-1]\n",
    "model_id = f'{base_model_name}-job-bias-qlora-seq2seq-cls'\n",
    "hub_model_id = f'{hf_site_id}/{model_id}'\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "bc121d59bab23a36",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "ddb7a63b9d9fcb8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:13.126214Z",
     "start_time": "2024-07-16T12:40:08.422856Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_id)\n",
    "column_names = dataset['train'].column_names\n",
    "\n",
    "text_col = 'text'\n",
    "label_cols = [col for col in column_names if col.startswith('label_')]\n",
    "\n",
    "labels = [label.replace(\"label_\", \"\") for label in label_cols]\n",
    "\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# Remove all columns apart from the two needed for multi-class classification\n",
    "keep_columns = ['id', text_col] + label_cols\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    dataset[split] = dataset[split].remove_columns(\n",
    "        [col for col in dataset[split].column_names if col not in keep_columns])\n",
    "\n",
    "for type in ['train', 'val', 'test']:\n",
    "    dataset[type] = dataset[type].shuffle(seed=seed)#.select(range(1000))\n",
    "\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'label_age', 'label_disability', 'label_feminine', 'label_general', 'label_masculine', 'label_neutral', 'label_racial', 'label_sexuality', 'text'],\n",
       "        num_rows: 4609\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'label_age', 'label_disability', 'label_feminine', 'label_general', 'label_masculine', 'label_neutral', 'label_racial', 'label_sexuality', 'text'],\n",
       "        num_rows: 593\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'label_age', 'label_disability', 'label_feminine', 'label_general', 'label_masculine', 'label_neutral', 'label_racial', 'label_sexuality', 'text'],\n",
       "        num_rows: 584\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "8f86fe66f2538e41",
   "metadata": {},
   "source": [
    "# Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "id": "df1730b90a1dcc7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:15.683350Z",
     "start_time": "2024-07-16T12:40:13.129071Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=HfFolder.get_token())\n",
    "#tokenizer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0706111e6a049f98c8b1824883fcf93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43cd111d1b174583a63ef04713d7e761"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae2e49075a62496984bdd9644b34e360"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68db72a619a14b00a0e8d9a3f8de1ac0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "54abe108f979a066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:17.561762Z",
     "start_time": "2024-07-16T12:40:15.684797Z"
    }
   },
   "source": [
    "from datasets import concatenate_datasets\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "\n",
    "# Prepare target sequences for T5\n",
    "def create_target_sequence(example):\n",
    "    labels = [key.replace('label_', '') for key, value in example.items() if key.startswith('label_') and value]\n",
    "    labels = ','.join(labels)\n",
    "    labels = labels.strip()\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Add target sequence to the dataset\n",
    "dataset = dataset.map(lambda x: {'labels': create_target_sequence(x)},\n",
    "                      remove_columns=[col for col in dataset['train'].column_names if col.startswith('label_')])\n",
    "\n",
    "# Tokenise targets\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"labels\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5193 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44f66fd502184c22a23d0f33dec25434"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5193 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0de27d028c7c42669106e8ca813ae43c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 13\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2917c7f3b405da7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:17.564546Z",
     "start_time": "2024-07-16T12:40:17.562654Z"
    }
   },
   "source": [
    "#tokenized_targets[\"input_ids\"]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "98424c47a6f9843b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:17.572066Z",
     "start_time": "2024-07-16T12:40:17.565244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Activate 4-bit precision base model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute dtype for 4-bit base models\n",
    "    bnb_4bit_use_double_quant=True,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")"
   ],
   "id": "628843522b7fc2c6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "102c5e2fda51da82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:43:44.038523Z",
     "start_time": "2024-07-16T12:40:17.572746Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(base_model_id, dropout_rate=dropout_rate)\n",
    "\n",
    "def get_base_model():\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_base_model()\n",
    "\n",
    "#model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5225120652f94f8ba76fd670b99dbf7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c724268163c14552b44cc0eff176a550"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09293c83598446d19e574a09781e717d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3906ff7f874f4d3c92d9823b25ca4828"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5450d185515f4f34a001554fe9a7d8d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30e1876ae361440f8a4f4ab635d47580"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3b24e3e3ef8474b97e21281bbc7372a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:43:44.227067Z",
     "start_time": "2024-07-16T12:43:44.039386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "\n",
    "\n",
    "peft_trainable_parameters = \"\"\n",
    "if use_lora:\n",
    "    print('Using LoRA')\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,  #2, #16,\n",
    "        lora_alpha=32,  #, 16, #8\n",
    "        target_modules='all-linear',\n",
    "        #target_modules=['q', 'v'],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(model.print_trainable_parameters())\n",
    "    \n",
    "    peft_trainable_parameters = model.print_trainable_parameters()\n",
    "else:\n",
    "    # Freeze the pre-trained model's parameters\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.config.use_cache = False  # Silence the warnings.\n",
    "#model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "model"
   ],
   "id": "8eb52687b70879b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoRA\n",
      "trainable params: 9,437,184 || all params: 2,859,194,368 || trainable%: 0.33006444422319176\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 2048)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 2048)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 2048)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-23): 23 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                  (v): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
       "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "7b1c50728f2cbab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:43:44.230939Z",
     "start_time": "2024-07-16T12:43:44.228049Z"
    }
   },
   "source": [
    "model.config"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_name_or_path\": \"google/flan-t5-xl\",\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 5120,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 2048,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"gelu_new\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"n_positions\": 512,\n",
       "  \"num_decoder_layers\": 24,\n",
       "  \"num_heads\": 32,\n",
       "  \"num_layers\": 24,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 200,\n",
       "      \"min_length\": 30,\n",
       "      \"no_repeat_ngram_size\": 3,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"summarize: \"\n",
       "    },\n",
       "    \"translation_en_to_de\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to German: \"\n",
       "    },\n",
       "    \"translation_en_to_fr\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to French: \"\n",
       "    },\n",
       "    \"translation_en_to_ro\": {\n",
       "      \"early_stopping\": true,\n",
       "      \"max_length\": 300,\n",
       "      \"num_beams\": 4,\n",
       "      \"prefix\": \"translate English to Romanian: \"\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.42.3\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:40:04.303419Z",
     "start_time": "2024-07-16T12:40:04.275227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "modules = set()\n",
    "for name, module in model.named_modules():\n",
    "    l = name.split('.')[-1].strip()\n",
    "    if l and not any(i.isdigit() for i in l):\n",
    "        modules.add(l)\n",
    "\n",
    "modules"
   ],
   "id": "149e196541229198",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_135401/3752488362.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmodules\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnamed_modules\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0ml\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0ml\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misdigit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mmodules\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "98d8c8d87d17cd44",
   "metadata": {},
   "source": [
    "# Preprocessing/Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:14:35.714217Z",
     "start_time": "2024-07-16T12:14:33.477034Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, \\\n",
    "    classification_report\n",
    "import nltk\n",
    "from transformers import  DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from typing import List, Tuple\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def preprocess_function(sample: Dataset, padding: str = \"max_length\") -> dict:\n",
    "    \"\"\"Preprocess the dataset.\"\"\"\n",
    "    inputs = [item for item in sample[\"text\"]]\n",
    "    labels = [item for item in sample[\"labels\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_source_length, padding=padding, truncation=True\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=labels, max_length=max_target_length, padding=padding, truncation=True\n",
    "    )\n",
    "\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(la if la != tokenizer.pad_token_id else -100) for la in label]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def postprocess_text(labels: List[str], preds: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Helper function to postprocess text\"\"\"\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n",
    "    return labels, preds\n",
    "\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    \n",
    "    y_hat, y = eval_predictions\n",
    "    \n",
    "    # Replace -100 in the labels .\n",
    "    y = np.where(y != -100, y, tokenizer.pad_token_id)\n",
    "    \n",
    "    if isinstance(y_hat, tuple):\n",
    "        y_hat = y_hat[0]\n",
    "        \n",
    "    y_str = tokenizer.batch_decode(y, skip_special_tokens=True)\n",
    "    y_hat_str = tokenizer.batch_decode(y_hat, skip_special_tokens=True)\n",
    "\n",
    "    #print(f'y_str:decoded:::{y_str}')\n",
    "    #print(f'pred_flat:decoded:::{y_hat_str}')\n",
    "    \n",
    "    y_str, y_hat_str = postprocess_text( y_str, y_hat_str)\n",
    "    \n",
    "    #print(f'y_str:post:::{y_str}')\n",
    "    #print(f'y_hat_str:post:::{y_hat_str}')\n",
    "    \n",
    "    # Flatten the list of labels\n",
    "    true_flat = [label.strip() for sublist in [t.split(',') for t in y_str] for label in sublist]\n",
    "    #pred_flat = [label.strip() for sublist in [p.split(',') for p in y_hat_str] for label in sublist]\n",
    "    \n",
    "    #print(f'true_flat:::{true_flat}')\n",
    "    #print(f'pred_flat:::{pred_flat}')\n",
    "    \n",
    "    # Convert to binary format for multi-label metrics\n",
    "    unique_labels = list(set(true_flat))\n",
    "    #print(f'unique_labels:::{unique_labels}')\n",
    "    \n",
    "    # Remove the blank label (no bias)\n",
    "    unique_labels = list([label for label in unique_labels if label != '' and label is not None])\n",
    "    unique_labels = sorted(list(set(unique_labels)))\n",
    "    target_names=sorted(list(set(id2label.values())))\n",
    "    use_auc_roc = len(unique_labels) == len(target_names)\n",
    "\n",
    "    #print(f'unique_labels:::{unique_labels}')\n",
    "    #print(f'target_names:::{target_names}')\n",
    "    \n",
    "    y_true = [[1 if label in t else 0 for label in target_names] for t in y_str]\n",
    "    y_pred = [[1 if label in p else 0 for label in target_names] for p in y_hat_str]\n",
    "\n",
    "    #print(f'y_true:::{y_true}')\n",
    "    #print(f'y_pred:::{y_pred}')\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    \n",
    "    # return as dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    }\n",
    "    \n",
    "    for average in ['micro','macro','samples','weighted']:\n",
    "        metrics[f'f1_{average}'] = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "        metrics[f'precision_{average}'] = precision_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "        metrics[f'recall_{average}'] = recall_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "        if use_auc_roc:\n",
    "            metrics[f'roc_auc_{average}'] = roc_auc_score(y_true=y_true, y_score=y_pred, average=average)\n",
    "    \n",
    "    return metrics\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 13:14:34.112223: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-16 13:14:34.178007: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-16 13:14:34.178064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-16 13:14:34.185671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-16 13:14:34.207024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 13:14:35.061507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "24f54a078782e3d2",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "id": "62c7dcb25c557604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:14:36.160005Z",
     "start_time": "2024-07-16T12:14:35.715077Z"
    }
   },
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=learning_rate,\n",
    "    output_dir=results_output_dir,\n",
    "    #logging_dir=logging_dir,  # logging & evaluation strategies\n",
    "    #auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,  # Overflows with fp16\n",
    "    #report_to=\"tensorboard\",\n",
    "    #push_to_hub=True,\n",
    "    #hub_strategy=\"every_save\",\n",
    "    #hub_model_id=REPOSITORY_ID,\n",
    "    #hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"text\", \"labels\"]\n",
    ")\n",
    "print(f\"Keys of tokenized dataset: {list(encoded_dataset['train'].features)}\")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=label_pad_token_id, pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "\n",
    "#early_stop = transformers.EarlyStoppingCallback(10, 1.15)\n",
    "class PrintClassificationCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/593 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dcff0ea86ad4e47ae96fc4c9a6d589e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['id', 'labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/teveritt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "3ff9c2bc-60a3-4b1c-8d09-708d67538000",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:14:36.471876Z",
     "start_time": "2024-07-16T12:14:36.160751Z"
    }
   },
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrintClassificationCallback]\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # Silence the warnings.\n",
    "\n",
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 13:14:36 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A2000 12GB          On  | 00000000:1C:00.0 Off |                  Off |\r\n",
      "| 30%   59C    P2              26W /  70W |   3359MiB / 12282MiB |      2%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      3586      G   /usr/lib/xorg/Xorg                            4MiB |\r\n",
      "|    0   N/A  N/A      4520      C   /usr/NX/bin/nxnode.bin                      139MiB |\r\n",
      "|    0   N/A  N/A     19525      C   /usr/bin/python3                           2748MiB |\r\n",
      "|    0   N/A  N/A    133185      C   ...eritt/local/venv/dcu-ai/bin/python3      454MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "023a8c2b-8df1-44be-b5cc-8e0db55c4a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:31:35.511552Z",
     "start_time": "2024-07-16T12:14:36.473073Z"
    }
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-16 13:14:36,701] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1731' max='1731' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1731/1731 16:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>Roc Auc Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Roc Auc Macro</th>\n",
       "      <th>F1 Samples</th>\n",
       "      <th>Precision Samples</th>\n",
       "      <th>Recall Samples</th>\n",
       "      <th>Roc Auc Samples</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "      <th>Roc Auc Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.472800</td>\n",
       "      <td>1.238281</td>\n",
       "      <td>0.123103</td>\n",
       "      <td>0.305998</td>\n",
       "      <td>0.251509</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.604669</td>\n",
       "      <td>0.319589</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.604669</td>\n",
       "      <td>0.292074</td>\n",
       "      <td>0.248454</td>\n",
       "      <td>0.401349</td>\n",
       "      <td>0.609959</td>\n",
       "      <td>0.319589</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.604669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.839900</td>\n",
       "      <td>1.181641</td>\n",
       "      <td>0.111298</td>\n",
       "      <td>0.310174</td>\n",
       "      <td>0.257202</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.607350</td>\n",
       "      <td>0.325875</td>\n",
       "      <td>0.320083</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.607350</td>\n",
       "      <td>0.287071</td>\n",
       "      <td>0.240585</td>\n",
       "      <td>0.402192</td>\n",
       "      <td>0.612963</td>\n",
       "      <td>0.325875</td>\n",
       "      <td>0.320083</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.607350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>1.195312</td>\n",
       "      <td>0.124789</td>\n",
       "      <td>0.306202</td>\n",
       "      <td>0.261013</td>\n",
       "      <td>0.370312</td>\n",
       "      <td>0.603407</td>\n",
       "      <td>0.320832</td>\n",
       "      <td>0.322988</td>\n",
       "      <td>0.370312</td>\n",
       "      <td>0.603407</td>\n",
       "      <td>0.280109</td>\n",
       "      <td>0.239741</td>\n",
       "      <td>0.380270</td>\n",
       "      <td>0.608185</td>\n",
       "      <td>0.320832</td>\n",
       "      <td>0.322988</td>\n",
       "      <td>0.370312</td>\n",
       "      <td>0.603407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.51      0.53      0.52        80\n",
      "  disability       0.26      0.51      0.35        80\n",
      "    feminine       0.18      0.38      0.25        80\n",
      "     general       0.15      0.54      0.23        80\n",
      "   masculine       0.09      0.03      0.04        80\n",
      "     neutral       0.16      0.38      0.22        80\n",
      "      racial       0.87      0.60      0.71        80\n",
      "   sexuality       0.39      0.17      0.24        80\n",
      "\n",
      "   micro avg       0.25      0.39      0.31       640\n",
      "   macro avg       0.33      0.39      0.32       640\n",
      "weighted avg       0.33      0.39      0.32       640\n",
      " samples avg       0.25      0.40      0.29       640\n",
      "\n",
      "----------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.51      0.54      0.52        80\n",
      "  disability       0.33      0.47      0.39        80\n",
      "    feminine       0.15      0.30      0.20        80\n",
      "     general       0.14      0.59      0.23        80\n",
      "   masculine       0.12      0.05      0.07        80\n",
      "     neutral       0.15      0.28      0.19        80\n",
      "      racial       0.89      0.78      0.83        80\n",
      "   sexuality       0.27      0.12      0.17        80\n",
      "\n",
      "   micro avg       0.26      0.39      0.31       640\n",
      "   macro avg       0.32      0.39      0.33       640\n",
      "weighted avg       0.32      0.39      0.33       640\n",
      " samples avg       0.24      0.40      0.29       640\n",
      "\n",
      "----------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.54      0.49      0.51        80\n",
      "  disability       0.41      0.45      0.43        80\n",
      "    feminine       0.15      0.20      0.17        80\n",
      "     general       0.15      0.68      0.24        80\n",
      "   masculine       0.11      0.05      0.07        80\n",
      "     neutral       0.15      0.23      0.18        80\n",
      "      racial       0.90      0.76      0.82        80\n",
      "   sexuality       0.18      0.11      0.14        80\n",
      "\n",
      "   micro avg       0.26      0.37      0.31       640\n",
      "   macro avg       0.32      0.37      0.32       640\n",
      "weighted avg       0.32      0.37      0.32       640\n",
      " samples avg       0.24      0.38      0.28       640\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1731, training_loss=1.354446807808438, metrics={'train_runtime': 1018.5947, 'train_samples_per_second': 13.575, 'train_steps_per_second': 1.699, 'total_flos': 9543300967563264.0, 'train_loss': 1.354446807808438, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "1b764b5923cd4155",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "id": "5bddaea63cf3ddc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:32:20.266522Z",
     "start_time": "2024-07-16T12:31:35.512328Z"
    }
   },
   "source": [
    "test_results = trainer.evaluate(eval_dataset=encoded_dataset['test'])\n",
    "test_results"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.48      0.39      0.43        80\n",
      "  disability       0.37      0.47      0.42        80\n",
      "    feminine       0.09      0.16      0.12        80\n",
      "     general       0.16      0.64      0.25        80\n",
      "   masculine       0.34      0.14      0.20        80\n",
      "     neutral       0.16      0.30      0.21        80\n",
      "      racial       0.84      0.82      0.83        80\n",
      "   sexuality       0.14      0.09      0.11        80\n",
      "\n",
      "   micro avg       0.25      0.38      0.30       640\n",
      "   macro avg       0.32      0.38      0.32       640\n",
      "weighted avg       0.32      0.38      0.32       640\n",
      " samples avg       0.24      0.39      0.28       640\n",
      "\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1513671875,\n",
       " 'eval_accuracy': 0.11301369863013698,\n",
       " 'eval_f1_micro': 0.3027638190954774,\n",
       " 'eval_precision_micro': 0.25315126050420167,\n",
       " 'eval_recall_micro': 0.3765625,\n",
       " 'eval_roc_auc_micro': 0.6001116071428572,\n",
       " 'eval_f1_macro': 0.3191085071327211,\n",
       " 'eval_precision_macro': 0.3218026766059445,\n",
       " 'eval_recall_macro': 0.3765625,\n",
       " 'eval_roc_auc_macro': 0.6001116071428572,\n",
       " 'eval_f1_samples': 0.28042237442922374,\n",
       " 'eval_precision_samples': 0.23715753424657535,\n",
       " 'eval_recall_samples': 0.39140981735159813,\n",
       " 'eval_roc_auc_samples': 0.6073630136986302,\n",
       " 'eval_f1_weighted': 0.3191085071327211,\n",
       " 'eval_precision_weighted': 0.3218026766059445,\n",
       " 'eval_recall_weighted': 0.3765625,\n",
       " 'eval_roc_auc_weighted': 0.6001116071428572,\n",
       " 'eval_runtime': 44.7483,\n",
       " 'eval_samples_per_second': 13.051,\n",
       " 'eval_steps_per_second': 1.631,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "dc9f308a-158f-4193-8fe9-3858c7147d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:33:05.171682Z",
     "start_time": "2024-07-16T12:32:20.267302Z"
    }
   },
   "source": [
    "y_true = dataset[\"test\"].map(\n",
    "    lambda x: tokenizer(x[\"labels\"], truncation=True, max_length=13, padding='max_length'),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "y_true = y_true['input_ids']\n",
    "y_true = np.where(y_true != -100, y_true, tokenizer.pad_token_id)\n",
    "\n",
    "predictions = trainer.predict(encoded_dataset['test'])\n",
    "y_pred = predictions.predictions\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/584 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77c62061e5ae44878aeb05750d14a181"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         age       0.48      0.39      0.43        80\n",
      "  disability       0.37      0.47      0.42        80\n",
      "    feminine       0.09      0.16      0.12        80\n",
      "     general       0.16      0.64      0.25        80\n",
      "   masculine       0.34      0.14      0.20        80\n",
      "     neutral       0.16      0.30      0.21        80\n",
      "      racial       0.84      0.82      0.83        80\n",
      "   sexuality       0.14      0.09      0.11        80\n",
      "\n",
      "   micro avg       0.25      0.38      0.30       640\n",
      "   macro avg       0.32      0.38      0.32       640\n",
      "weighted avg       0.32      0.38      0.32       640\n",
      " samples avg       0.24      0.39      0.28       640\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "ca4710ca-74ec-4f55-852a-4a432f237292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T00:00:18.219191Z",
     "start_time": "2024-07-17T00:00:18.194915Z"
    }
   },
   "source": [
    "\n",
    "y_str = tokenizer.batch_decode(y_true, skip_special_tokens=True)\n",
    "y_hat_str = tokenizer.batch_decode(y_pred, skip_special_tokens=True)\n",
    "\n",
    "y_str, y_hat_str = postprocess_text(y_str, y_hat_str)\n",
    "\n",
    "# Flatten the list of labels\n",
    "true_flat = [label.strip() for sublist in [t.split(',') for t in y_str] for label in sublist]\n",
    "pred_flat = [label.strip() for sublist in [p.split(',') for p in y_hat_str] for label in sublist]\n",
    "\n",
    "target_names=sorted(list(set(id2label.values())))\n",
    "\n",
    "y_true = [[1 if label in t else 0 for label in target_names] for t in y_str]\n",
    "y_pred = [[1 if label in p else 0 for label in target_names] for p in y_hat_str]\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "\n",
    "#print(report)\n",
    "\n",
    "# Convert to Markdown\n",
    "report_lines = report.split('\\n')\n",
    "markdown_classification_report = \"\\n\".join([f\"    {line}\" for line in report_lines])\n",
    "print(markdown_classification_report)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_192713/287404553.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0my_str\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskip_special_tokens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0my_hat_str\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskip_special_tokens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0my_str\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_hat_str\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpostprocess_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_str\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_hat_str\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3d29aa2b5692bf13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:33:05.212608Z",
     "start_time": "2024-07-16T12:33:05.195012Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list(test_results.items()), columns=['Metric', 'Value'])\n",
    "print(df.to_string(index=False))\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list(test_results.items()), columns=['Metric', 'Value'])\n",
    "print(df.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric     Value\n",
      "              eval_loss  1.151367\n",
      "          eval_accuracy  0.113014\n",
      "          eval_f1_micro  0.302764\n",
      "   eval_precision_micro  0.253151\n",
      "      eval_recall_micro  0.376563\n",
      "     eval_roc_auc_micro  0.600112\n",
      "          eval_f1_macro  0.319109\n",
      "   eval_precision_macro  0.321803\n",
      "      eval_recall_macro  0.376563\n",
      "     eval_roc_auc_macro  0.600112\n",
      "        eval_f1_samples  0.280422\n",
      " eval_precision_samples  0.237158\n",
      "    eval_recall_samples  0.391410\n",
      "   eval_roc_auc_samples  0.607363\n",
      "       eval_f1_weighted  0.319109\n",
      "eval_precision_weighted  0.321803\n",
      "   eval_recall_weighted  0.376563\n",
      "  eval_roc_auc_weighted  0.600112\n",
      "           eval_runtime 44.748300\n",
      "eval_samples_per_second 13.051000\n",
      "  eval_steps_per_second  1.631000\n",
      "                  epoch  3.000000\n",
      "                 Metric     Value\n",
      "              eval_loss  1.151367\n",
      "          eval_accuracy  0.113014\n",
      "          eval_f1_micro  0.302764\n",
      "   eval_precision_micro  0.253151\n",
      "      eval_recall_micro  0.376563\n",
      "     eval_roc_auc_micro  0.600112\n",
      "          eval_f1_macro  0.319109\n",
      "   eval_precision_macro  0.321803\n",
      "      eval_recall_macro  0.376563\n",
      "     eval_roc_auc_macro  0.600112\n",
      "        eval_f1_samples  0.280422\n",
      " eval_precision_samples  0.237158\n",
      "    eval_recall_samples  0.391410\n",
      "   eval_roc_auc_samples  0.607363\n",
      "       eval_f1_weighted  0.319109\n",
      "eval_precision_weighted  0.321803\n",
      "   eval_recall_weighted  0.376563\n",
      "  eval_roc_auc_weighted  0.600112\n",
      "           eval_runtime 44.748300\n",
      "eval_samples_per_second 13.051000\n",
      "  eval_steps_per_second  1.631000\n",
      "                  epoch  3.000000\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "843e92a89b8b7b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:33:05.224847Z",
     "start_time": "2024-07-16T12:33:05.213712Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def classify_text(text, model, tokenizer, label_columns, device):\n",
    "    input_text = f\"classify: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    predicted_labels = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicted_labels = [label.strip() for label in predicted_labels.split(',')]\n",
    "    label_dict = {label: False for label in label_columns}\n",
    "    for label in predicted_labels:\n",
    "        if label in label_dict:\n",
    "            label_dict[label] = True\n",
    "    return label_dict"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "d76dd95a6c47d93f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:33:05.315632Z",
     "start_time": "2024-07-16T12:33:05.226231Z"
    }
   },
   "source": [
    "text = \"Looking for a native English speaker\"\n",
    "\n",
    "classify_text(text, model, tokenizer, labels, device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': False,\n",
       " 'disability': False,\n",
       " 'feminine': False,\n",
       " 'general': False,\n",
       " 'masculine': False,\n",
       " 'neutral': False,\n",
       " 'racial': False,\n",
       " 'sexuality': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "d3b121d2c658404",
   "metadata": {},
   "source": [
    "# Push to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "id": "c44cec6c9abb4937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T12:33:11.416463Z",
     "start_time": "2024-07-16T12:33:05.316793Z"
    }
   },
   "source": [
    "model.push_to_hub(repo_id=hub_model_id, token=HfFolder.get_token())\n",
    "tokenizer.push_to_hub(repo_id=hub_model_id, token=HfFolder.get_token())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merge Adapter and Base Model",
   "id": "ee398b933669036a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get peft config.\n",
    "from peft import PeftConfig\n",
    "config = PeftConfig.from_pretrained(hub_model_id)\n",
    "\n",
    "# Get base model\n",
    "model = get_base_model()\n",
    "\n",
    "# Load the Lora model.\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, hub_model_id, torch_dtype=torch.bfloat16, is_trainable=False)\n",
    "\n",
    "# Merge model and Lora adapter.\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Push to HF Hub.\n",
    "merged_model.push_to_hub(hub_model_id)\n",
    "tokenizer.push_to_hub(hub_model_id)"
   ],
   "id": "c95056458e572a51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Update Model Card ",
   "id": "1b7618a9bb40210"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/7.10M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "686aebf256084e4a833120980203d38b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dda6bf42c3f84fc29aecc6991202f186"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a330144c7714e73bf4aed25a9681d3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/2024-mcm-everitt-ryan/flan-t5-base-job-bias-4bit-qlora-seq2seq-cls/commit/2d80218fb5c98a0fbcb364d97506db03cd98173c', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='2d80218fb5c98a0fbcb364d97506db03cd98173c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22,
   "source": [
    "from huggingface_hub import ModelCard, EvalResult, ModelCardData\n",
    "import platform\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "training_regime = []\n",
    "if use_lora:\n",
    "    training_regime.append(f'PEFT: {peft_trainable_parameters}')\n",
    "training_regime_args = args.to_sanitized_dict()\n",
    "for k,v in training_regime_args.items():\n",
    "    if (isinstance(v, (int, str, bool, float))\n",
    "            and '_dir' not in k\n",
    "            and 'logging' not in k\n",
    "            and 'log_' not in k\n",
    "            and 'hub_' not in k\n",
    "            and '_hub' not in k\n",
    "            and 'save_' not in k\n",
    "            and 'run_name' not in k\n",
    "            and 'debug' not in k\n",
    "            and 'token' not in k):\n",
    "        training_regime.append(f'{k}={json.dumps(v)}')\n",
    "\n",
    "training_regime = sorted(training_regime)\n",
    "\n",
    "training_regime = ', '.join(training_regime)\n",
    "\n",
    "\n",
    "## Hardware\n",
    "compute_infrastructure = []\n",
    "mem_total = !cat /proc/meminfo | grep MemTotal\n",
    "mem_total = list(set(mem_total))[0]\n",
    "cpu_info = !cat /proc/cpuinfo | grep \"model name\"\n",
    "cpu_count = len(list(cpu_info))\n",
    "cpu_name = list(set(cpu_info))[0]\n",
    "cpu_name = cpu_name.strip()\n",
    "cpu_name = cpu_name.replace('model name\\t:', '')\n",
    "cpu_name = cpu_name.strip()\n",
    "\n",
    "compute_infrastructure.append(f'- {platform.system()} {platform.release()} {platform.processor()}')\n",
    "compute_infrastructure.append(f'- {mem_total}')\n",
    "compute_infrastructure.append(f'- {cpu_count} X {cpu_name}')\n",
    "\n",
    "\n",
    "gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv,noheader\n",
    "gpus = set()\n",
    "for idx, gpu in enumerate(gpu_name):\n",
    "    compute_infrastructure.append(f\"- GPU_{idx}: {gpu}\")\n",
    "    gpus.add(gpu)\n",
    "\n",
    "gpus =list(gpus)\n",
    "compute_infrastructure = '\\n'.join(compute_infrastructure)\n",
    "hardware_type = f'{len(gpus)} X {gpus[0]}'\n",
    "\n",
    "## Software\n",
    "software_list = !pip list\n",
    "inc_software = []\n",
    "inc_software.append(f'python {platform.python_version()}')\n",
    "\n",
    "for software in software_list:\n",
    "    if software and '[notice]' not in software and '---' not in software and 'Package' not in software:\n",
    "        inc_software.append(' '.join(software.split()))\n",
    " \n",
    "\n",
    "software = \", \".join(inc_software)   \n",
    "\n",
    "hours_used = \"\"\n",
    "eval_results = []\n",
    "for k, v in test_results.items():\n",
    "    metric_type = k.replace(\"eval_\", \"\", 1)\n",
    "    if metric_type == 'runtime':\n",
    "        hours_used = f\"{int(v)/60.0:.2f}\"\n",
    "    eval_results.append(EvalResult(\n",
    "        task_type='multi_label_classification',\n",
    "        dataset_type='mix_human-eval_synthetic',\n",
    "        dataset_name=dataset_id,\n",
    "        metric_type=metric_type,\n",
    "        metric_value=v))\n",
    "\n",
    "direct_use = \"\"\"\n",
    "    ```python\n",
    "    from transformers import pipeline\n",
    "\n",
    "    pipe = pipeline(\"text-classification\", model=\"${hub_model_id}\", return_all_scores=True)\n",
    "\n",
    "    results = pipe(\"Join our dynamic and fast-paced team as a Junior Marketing Specialist. We seek a tech-savvy and energetic individual who thrives in a vibrant environment. Ideal candidates are digital natives with a fresh perspective, ready to adapt quickly to new trends. You should have recent experience in social media strategies and a strong understanding of current digital marketing tools. We're looking for someone with a youthful mindset, eager to bring innovative ideas to our young and ambitious team. If you're a recent graduate or early in your career, this opportunity is perfect for you!\")\n",
    "    print(results)\n",
    "    ```\n",
    "    >> [[\n",
    "    {'label': 'age', 'score': 0.9883460402488708}, \n",
    "    {'label': 'disability', 'score': 0.00787709467113018}, \n",
    "    {'label': 'feminine', 'score': 0.007224376779049635}, \n",
    "    {'label': 'general', 'score': 0.09967829287052155}, \n",
    "    {'label': 'masculine', 'score': 0.0035264550242573023}, \n",
    "    {'label': 'racial', 'score': 0.014618005603551865}, \n",
    "    {'label': 'sexuality', 'score': 0.005568435415625572}\n",
    "    ]]\n",
    "    \"\"\"\n",
    "\n",
    "direct_use = direct_use.replace('${hub_model_id}', hub_model_id, -1)\n",
    "\n",
    "card_data = ModelCardData(\n",
    "    model_id=model_id,\n",
    "    model_name=model_id,\n",
    "    model_description=\"The model is a multi-label classifier designed to detect various types of bias within job descriptions.\",\n",
    "    base_model=base_model_id,\n",
    "    language='en',\n",
    "    license='apache-2.0',\n",
    "    developers=\"Tristan Everitt and Paul Ryan\",\n",
    "    model_card_authors='See developers',\n",
    "    model_card_contact='See developers',\n",
    "    repo=\"https://gitlab.computing.dcu.ie/everitt2/2024-mcm-everitt-ryan\",\n",
    "    training_regime=training_regime,\n",
    "    eval_results=eval_results,\n",
    "    results=markdown_classification_report,\n",
    "    compute_infrastructure=compute_infrastructure,\n",
    "    # hardware_requirements='N/A',\n",
    "    software=software,\n",
    "    hardware_type=hardware_type,\n",
    "    hours_used=hours_used,\n",
    "    cloud_provider='N/A',\n",
    "    cloud_region='N/A',\n",
    "    co2_emitted='N/A',\n",
    "    datasets=[dataset_id],\n",
    "    direct_use=direct_use\n",
    ")\n",
    "\n",
    "card = ModelCard.from_template(card_data)\n",
    "\n",
    "card.push_to_hub(repo_id=hub_model_id, token=HfFolder.get_token())"
   ],
   "id": "5033d518-b2be-40af-9aa9-be9de9729f5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DCU AI)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
